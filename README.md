# model_evaluation
LLM-Auto-Evaluator 是一个用于自动化评估本地大语言模型（LLMs）能力的框架。它通过一个更强大的在线“裁判”模型（例如 GPT-4o, Gemini 等），对本地模型（通过 Ollama 部署）的性能进行系统性、多维度、数据驱动的评估。
